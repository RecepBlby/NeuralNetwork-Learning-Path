{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neurons.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwFhcRIcb-s2"
      },
      "source": [
        "Every day we witness the awesome powers of neural networks, they detect features, solve problems, and make you look like a mad scientist. So why are they so strong? Short answer; because they gain their strength from the simplicity of their structure.\r\n",
        "\r\n",
        "# Functions\r\n",
        "Imagine a small box, you throw something into the box and you get some results from the box. Quite simply, I'm sure you know what I'm talking about, actually this box is a mathematical function. image.png You have some inputs in your hand, and you define and run a function that will generate output from these inputs, now you will have outputs or results.What if we had inputs and we also had outputs? Then the missing piece of the puzzle would be the function itself, right?So let's look at the problem from a different window and design the box that will produce the output we have from the inputs. Perceptrons help solve this problem.\r\n",
        "\r\n",
        "# Perceptron \r\n",
        "is a mathematical model that multiplies the values you give to its inputs by a weighting coefficient and adds a bias value, comparing the output you expect to be with your output value, and updating the relevant weight and bias parameters to get the desired output.\r\n",
        "\r\n",
        "Let's think of a scenario that will stop thinking a little more abstractly and simplify their understanding immediately and bring it to life.Take the issue of propositions you might remember from the topic of logic, two of which should suggest \"and\" and \"or\". image.png Let's look at the table, we have inputs and we also have outputs that vary according to these inputs, hopefully it tells you the same thing. Perceptrons. Now let's jump to the more fun side of things and solve this problem with a very simple single layer neural network structure, which we call perceptron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qaXV7lJcxeQ"
      },
      "source": [
        "Our perceptron will be a simple structure with 2 neurons in the first layer and only 1 neuron at the output, this will be enough to solve the \"and gate\" problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QRzlSEaceEY",
        "outputId": "8783e926-b257-4b20-ebce-904771c2975a"
      },
      "source": [
        "import keras\r\n",
        "import numpy as np\r\n",
        "#Let's define the inputs we have and the outputs of these inputs.\r\n",
        "xs_and = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]], dtype = float)\r\n",
        "ys_and = np.array([[0.0], [0.0], [0.0], [1.0]], dtype = float)\r\n",
        "\r\n",
        "model_and = keras.Sequential()\r\n",
        "model_and.add(keras.layers.Dense(units=2, input_shape=[2]))\r\n",
        "model_and.add(keras.layers.Dense(units=1))\r\n",
        "\r\n",
        "model_and.compile(optimizer=\"sgd\", loss=\"mean_squared_error\", metrics=[\"accuracy\"])\r\n",
        "\r\n",
        "model_and.fit(xs_and, ys_and, epochs=300, verbose=0)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa3830f7940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ahYx-oc1o3"
      },
      "source": [
        "Let's run the model and see if our neural network really created the structure that will give the right result for us. Let's send \"0\" and \"0\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEtL8vSMc2RG",
        "outputId": "291a84e0-b10e-4164-e55a-c36ea614baeb"
      },
      "source": [
        "model_and.predict(np.array([[0.0, 0.0]]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.23038407]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GqOUFK9c5Az"
      },
      "source": [
        "Let's define possible entries in our table to variables, then let's try to print all my inputs by writing a simple for loop which output will give each"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-loYfziwc5wC",
        "outputId": "03c10897-0e45-4bf2-8ee8-57026ee4a24f"
      },
      "source": [
        "pred_zero_zero = np.array([[0.0,0.0]]) # Expecting False\r\n",
        "pred_zero_one = np.array([[0.0,1.0]])\r\n",
        "pred_one_zero = np.array([[1.0,0.0]])\r\n",
        "pred_one_one = np.array([[1.0,1.0]])\r\n",
        "predictions = [pred_zero_zero, pred_zero_one, pred_one_zero, pred_one_one]\r\n",
        "\r\n",
        "for pred in predictions:\r\n",
        "    print(model_and.predict(pred))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.23038407]]\n",
            "[[0.23535606]]\n",
            "[[0.26852098]]\n",
            "[[0.73426116]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LG2rxi7c6_G"
      },
      "source": [
        "What would I get if I set a threshold value here and set it to be \"1\" when the output is greater than 0.5 and \"0\" when it is small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcMmkoLBc8nt",
        "outputId": "85e443cc-1d9d-45dc-a850-4fb1742f4aa8"
      },
      "source": [
        "def prediction_and(pred):\r\n",
        "    x = model_and.predict(pred)\r\n",
        "    print(x)\r\n",
        "    if x > 0.5:\r\n",
        "        print(\"Correct\\n\")\r\n",
        "    else:\r\n",
        "        print(\"False\\n\")\r\n",
        "\r\n",
        "for pred in predictions:\r\n",
        "    prediction_and(pred)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.23038407]]\n",
            "False\n",
            "\n",
            "[[0.23535606]]\n",
            "False\n",
            "\n",
            "[[0.26852098]]\n",
            "False\n",
            "\n",
            "[[0.73426116]]\n",
            "Correct\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42La1-kTc_fj"
      },
      "source": [
        "We solved the \"and gate\" problem with a simple perceptron or a single layer neural network.\r\n",
        "\r\n",
        "How can we put it on paper with a simple calculator? To do this, we need the weight and bias values within our neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWfvKok2c9__",
        "outputId": "24f76cfb-b9e4-41e9-a0dd-6cfd43084509"
      },
      "source": [
        "first_layer_weights_and = model_and.layers[0].get_weights()[0]\r\n",
        "first_layer_biases_and  = model_and.layers[0].get_weights()[1]\r\n",
        "print(\"FIRST LAYER WEIGHTS\")\r\n",
        "print(first_layer_weights_and)\r\n",
        "print(\"\\nFIRST LAYER BIASES\")\r\n",
        "print(first_layer_biases_and)\r\n",
        "\r\n",
        "second_layer_weights_and = model_and.layers[1].get_weights()[0]\r\n",
        "second_layer_biases_and  = model_and.layers[1].get_weights()[1]\r\n",
        "print(\"\\nOUTPUT LAYER WEIGHT\")\r\n",
        "print(second_layer_weights_and)\r\n",
        "print(\"\\n OUTPUT LAYER BIAS\")\r\n",
        "print(second_layer_biases_and)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FIRST LAYER WEIGHTS\n",
            "[[-0.53747416  1.0522358 ]\n",
            " [-0.36312664 -0.56071895]]\n",
            "\n",
            "FIRST LAYER BIASES\n",
            "[ 0.1199667  -0.02607459]\n",
            "\n",
            "OUTPUT LAYER WEIGHT\n",
            "[[-1.126337  ]\n",
            " [-0.10118645]]\n",
            "\n",
            " OUTPUT LAYER BIAS\n",
            "[-0.09789953]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1GrMRegdE8Y"
      },
      "source": [
        "We multiply the inputs to each neuron by their weight and add them all together, then add the bias.Let's apply [1,1] to the inputs.\r\n",
        "\r\n",
        "The value of the first neuron in the Hidden Layer: [[1.0 x 0.1698] + [1.0 x -0.7285]] + [-0.0383] = -0.5979\r\n",
        "\r\n",
        "The value of the second neuron in the Hidden Layer: [[1.0 x 0.6799] + [1.0 x -0.1077]] + [-0.0391] = 0.5331\r\n",
        "\r\n",
        "The value of the neuron in the Output Layer: z3 = [[-0.5979 x -0.500] + [0.5331 x 0.354]] + [0.0311] = 0.517"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMuuII3UVp2w"
      },
      "source": [
        "**Convolutional Neural Networks**\r\n",
        "---\r\n",
        "A convolutional neural network is a feed-forward neural network that is generally used to analyze visual images by processing data with grid-like topology. It’s also known as a ConvNet. A convolutional neural network is used to detect and classify objects in an image. \r\n",
        "\r\n",
        "Traditional neural networks called the multilayer perceptron (MLP) are modeled on the human brain, whereby neurons are stimulated by connected nodes and are only activated when a certain threshold value is reached.\r\n",
        "There are several drawbacks of MLP’s, especially when it comes to image processing. MLPs use one perceptron for each input (e.g. pixel in an image, multiplied by 3 in RGB case). The amount of weights rapidly becomes unmanageable for large images. For a 224 x 224 pixel image with 3 color channels there are around 150,000 weights that must be trained!\r\n",
        "\r\n",
        "Computers ‘see’ in a different way than we do. Their world consists of only numbers. Every image can be represented as 2-dimensional arrays of numbers, known as pixels. But the fact that they perceive images in a different way, doesn’t mean we can’t train them to recognize patterns, like we do. We just have to think of what an image is in a different way. To teach an algorithm how to recognise objects in images, we use a specific type of Artificial Neural Network: a Convolutional Neural Network (CNN). Their name stems from one of the most important operations in the network: convolution.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPXmMP0WWET7"
      },
      "source": [
        "The concept of hierarchy plays a significant role in the brain. Information is stored in sequences of patterns, in sequential order. The neocortex, which is the outermost layer of the brain, stores information hierarchically. It is stored in cortical columns, or uniformly organised groupings of neurons in the neocortex.\r\n",
        "\r\n",
        "Regular Neural Networks transform an input by putting it through a series of hidden layers. Every layer is made up of a set of neurons, where each layer is fully connected to all neurons in the layer before. Finally, there is a last fully-connected layer — the output layer — that represent the predictions.\r\n",
        "\r\n",
        "Convolutional Neural Networks are a bit different. First of all, the layers are organised in 3 dimensions: width, height and depth. Further, the neurons in one layer do not connect to all the neurons in the next layer but only to a small region of it. Lastly, the final output will be reduced to a single vector of probability scores, organized along the depth dimension.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7v86Lx8WHTo"
      },
      "source": [
        "CNNs have two components: \r\n",
        "* The Hidden layers/Feature extraction part\r\n",
        "In this part, the network will perform a series of convolutions and pooling operations during which the features are detected. If you had a picture of a zebra, this is the part where the network would recognise its stripes, two ears, and four legs.\r\n",
        "* The Classification part \r\n",
        "Here, the fully connected layers will serve as a classifier on top of these extracted features. They will assign a probability for the object on the image being what the algorithm predicts it is.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx1h2bDUYjoe"
      },
      "source": [
        "Convolution is one of the main building blocks of a CNN. The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information.\r\n",
        "\r\n",
        "CNN, the convolution is performed on the input data with the use of a filter or kernel to then produce a feature map. We execute a convolution by sliding the filter over the input. At every location, a matrix multiplication is performed and sums the result onto the feature map. The area of our filter is also called the receptive field, named after the neuron cells!\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iogzsXfYpnl"
      },
      "source": [
        "We perfom numerous convolutions on our input, where each operation uses a different filter. This results in different feature maps. In the end, we take all of these feature maps and put them together as the final output of the convolution layer. Just like any other Neural Network, we use an activation function to make our output non-linear. In the case of a Convolutional Neural Network, the output of the convolution will be passed through the activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewkc50ToVZ8r"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhwts5U9Vb-i"
      },
      "source": [
        "from keras.layers import Conv2D, Activation, MaxPool2D, Flatten, Dense\r\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRQglJLdWI6X"
      },
      "source": [
        "# Images fed into this model are 512 x 512 pixels with 3 channels\r\n",
        "img_shape = (28,28,1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvg0WAlzYNIw"
      },
      "source": [
        "# Set up the model\r\n",
        "model = Sequential()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixNXaVWeYQCS"
      },
      "source": [
        "# Add convolutional layer with 3, 3 by 3 filters and a stride size of 1\r\n",
        "# Set padding so that input size equals output size\r\n",
        "model.add(Conv2D(6,2,input_shape=img_shape))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1cD6LBiYU8Y"
      },
      "source": [
        "# Add relu activation to the layer \r\n",
        "model.add(Activation('relu'))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re2h06HdYWua"
      },
      "source": [
        "#Pooling\r\n",
        "model.add(MaxPool2D(2))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnC3guZAYqld"
      },
      "source": [
        "#Fully connected layers\r\n",
        "# Use Flatten to convert 3D data to 1D\r\n",
        "model.add(Flatten())\r\n",
        "# Add dense layer with 10 neurons\r\n",
        "model.add(Dense(10))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE6qMR7MZYM-"
      },
      "source": [
        "# we use the softmax activation function for our last layer\r\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7hnRE4uZbAl",
        "outputId": "3f1df392-c6f3-47bd-901c-3616875b2ad5"
      },
      "source": [
        "# give an overview of our model\r\n",
        "model.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 27, 27, 6)         30        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 27, 27, 6)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 6)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1014)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                10150     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 10,180\n",
            "Trainable params: 10,180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hNsUAaOZ-E-"
      },
      "source": [
        "\"\"\"Before the training process, we have to put together a learning process in a particular form. \r\n",
        "It consists of 3 elements: an optimiser, a loss function and a metric.\"\"\"\r\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics=['acc'])"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MqIWccNaCX_"
      },
      "source": [
        "# dataset with handwritten digits to train the model on\r\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMXrsXilaFlE"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaWyuYQYaIZq"
      },
      "source": [
        "x_train = np.expand_dims(x_train,-1)\r\n",
        "x_test = np.expand_dims(x_test,-1)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTy8O66OaJHt",
        "outputId": "65b35025-c363-496b-e58b-6a0d3aa4be9a"
      },
      "source": [
        "# Train the model, iterating on the data in batches of 32 samples# for 10 epochs\r\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 5.2645 - acc: 0.8048 - val_loss: 0.4286 - val_acc: 0.9384\n",
            "Epoch 2/10\n",
            "1360/1875 [====================>.........] - ETA: 4s - loss: 0.3235 - acc: 0.9408"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYrep0YjaOb0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}